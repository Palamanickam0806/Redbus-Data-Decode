# Redbus Data Decode Hackathon 2025

## Overview

This repository contains the solution for the **Redbus Data Decode Hackathon 2025** hosted by Analytics Vidhya. The objective of the hackathon was to predict the demand (total number of seats booked) for each journey at the route level, 15 days before the actual date of journey (DOJ). The solution leverages machine learning and deep learning models to achieve this goal.

**Hackathon Rank**: **310** out of 8,000+ participants.

## Problem Statement

The task was to predict the `final_seatcount` (total number of seats booked) for bus routes 15 days prior to the journey date (DOJ) based on historical transaction data. The dataset includes features such as:

- `doj`: Date of journey
- `doi`: Date of inquiry
- `srcid`, `destid`: Source and destination IDs
- `srcid_region`, `destid_region`: Source and destination regions
- `srcid_tier`, `destid_tier`: Source and destination tier classifications
- `cumsum_seatcount`: Cumulative seat count
- `cumsum_searchcount`: Cumulative search count
- `dbd`: Days before departure

The dataset spans multiple days before the journey (DBD ranging from 0 to 30), with the goal of predicting demand at DBD=15.

## Dataset

- **Training Data** (`train/transactions.csv`, `train/train.csv`): Contains historical transaction data and final seat counts for routes.
- **Test Data** (`data/test.csv`): Contains route information for which predictions are required.
- **Processed Data** (`data/train.csv`): Generated by preprocessing scripts to align with model requirements.
- **Size**: The filtered training data for DBD>=15 consists of approximately 1075000 rows. ( For only dbd = 15 data consist 67000 rows)

## Approach

### Data Preprocessing

- **Filtering**: Focused on DBD=15 for machine learning models to simplify the feature set and reduce dimensionality.
- **Feature Engineering**:
  - Added temporal features: `day_of_week` and `month` derived from `doj`.
  - Encoded categorical features (`srcid_region`, `destid_region`, `srcid_tier`, `destid_tier`) using `LabelEncoder`.
  - For deep learning, included a `search_to_book` ratio and used historical data for DBD ≥ 15.
- **Data Quality**:
  - Ensured complete time series for routes (16 records per route for DBD 15-30).
  - Handled missing values by filling with median `final_seatcount` where necessary.
- **Scripts**:
  - `data_preprocess.ipynb`: Initial data exploration and preprocessing.


### Machine Learning Models

Three machine learning models were implemented to predict `final_seatcount` using features from DBD=15 only:

1. **XGBoost** (`xgboost.py`):
   - Used `XGBRegressor` with parameters: `n_estimators=200`, `learning_rate=0.1`, `max_depth=6`.
   - Features: `srcid`, `destid`, `srcid_region`, `destid_region`, `srcid_tier`, `destid_tier`, `cumsum_seatcount`, `cumsum_searchcount`, `day_of_week`, `month`.
   - Limitation: Did not capture temporal dependencies due to single-day snapshot (DBD=15).
2. **LightGBM** (`lgbm.py`):
   - Used `lightgbm` with parameters: `learning_rate=0.05`, `num_leaves=64`, `max_depth=8`, `min_child_samples=30`.
   - Applied log transformation on the target (`final_seatcount`) and regularization for robustness.
   - **Performance**: Achieved the best machine learning RMSE of **678.54** on the test set.
3. **HistGradientBoostingRegressor**:
   - Tested as an alternative but not used in the final submission.

### Deep Learning Model

A deep learning approach was implemented to leverage temporal dependencies across DBD ≥ 15:

- **Model** (`deep.py`):
  - Used a Gated Recurrent Unit (GRU) with 5 layers and a hidden size of 128.
  - Input features:
    - **Sequence Features**: `dbd`, `cumsum_seatcount`, `cumsum_searchcount`, `search_to_book`, `day_of_week`, `month`.
    - **Static Features**: `srcid`, `destid`, `srcid_region`, `destid_region`, `srcid_tier`, `destid_tier`.
  - Architecture: GRU followed by fully connected layers (128 → 64 → 1).
  - Training: 80 epochs with Adam optimizer (`lr=1e-3`), batch size of 128, and MSE loss.
- **Data Preparation**:
  - Grouped data by route (`doj`, `srcid`, `destid`) to create sequences of 16 time steps (DBD 15-30).
  - Filtered routes with exactly 16 records to ensure consistent sequence length.
- **Performance**: Achieved a validation RMSE of **478**, significantly outperforming machine learning models due to its ability to model temporal patterns.

### Final Submission

- **Model Choice**: The Light Gradient Boosting  model was used for the final submission due to its superior performance.
- **Test Data Processing**:
  - Prepared test data by merging with transaction data at DBD=15.
  - Applied the same feature engineering and encoding as the training data.
  - Filled missing predictions with the median `final_seatcount` from the training data.
- **Output**: Generated `submission_ml.csv` with predicted `final_seatcount` for each `route_key`.

## Files

- `xgboost.py`: Implementation of the XGBoost model for DBD=15 data.
- `lgbm.py`: Implementation of the LightGBM model, best ML model with RMSE=678.54.
- `deep.py`: Implementation of the GRU-based deep learning model, achieving RMSE=478.
- `data_preprocess.ipynb`: Jupyter notebook for data exploration and preprocessing.
- `train.csv`: Processed training data with updated `final_seatcount`.

## Results

- **Validation Performance**:
  - XGBoost: Not specified (outperformed by LightGBM).
  - LightGBM: RMSE = 678.54.
  - GRU (Deep Learning): RMSE = 478.
- **Hackathon Rank**: 310 out of 8,000+ participants, demonstrating a competitive solution.


## Key Insights

- The deep learning model (GRU) outperformed traditional ML models by leveraging temporal dependencies across DBD 15-30.
- LightGBM was the best ML model, benefiting from log transformation and regularization and used for submission.
- Feature engineering (e.g., `day_of_week`, `month`, `search_to_book`) was critical for capturing demand patterns.
- Data quality checks ensured robust preprocessing, with no incomplete time series or duplicate entries.

## Future Improvements

- Explore additional temporal features, such as holidays or seasonal trends.
- Experiment with ensemble methods combining ML and deep learning predictions.
- Fine-tune GRU hyperparameters (e.g., layers, hidden size) for better performance.
- Incorporate more advanced sequence models like Transformers for improved temporal modeling.

## Acknowledgments

- Thanks to **Analytics Vidhya** for organizing the Redbus Data Decode Hackathon 2025.
- Gratitude to the open-source community for libraries like `xgboost`, `lightgbm`, and `pytorch`.